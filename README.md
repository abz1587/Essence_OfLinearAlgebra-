# 一、线性代数的直观理解

Welcome to the Essence_OfLinearAlgebra- wiki!

学习任何一个领域的知识，我认为有三点是最重要的，也是快速入门的捷径：一个是对基本概念的意义的理解，一个是该领域要解决的问题，一个是解决该问题的思路。 数学如此，工程如此，人文艺术也是如此。

我一直认为，数学是人类的一种思维方式，是和人类大脑的构造和运作机制相关的，因为人类是用人脑的构造和机制去理解世界，所以数学成为人类理解世界的语言。

数学里面很多概念、问题、思路都来源于实际的工程问题，所以，理解数学概念的意义最为重要，而数学的计算技巧并不重要（尤其现在有了计算机之后）。可惜的是，很多数学教材（尤其是国内教材）都把重点放在了如何计算上，而不是如何理解概念和公式的意义上。

我曾经阅读过加拿大安省12年级的《Advanced Mathematics》，里面对于Vector的讲解，就是从场景和问题出发，逐步引入Vector的概念，花了大量篇幅让学生探索和理解Vector的几何意义和物理意义，然后让学生学会用计算器进行Vector的计算，并没有让学生进行大量的手算训练。我很推崇这种学习方法，甚至认为，如果当年的高等数学、软件工程等都是用这种方法去讲，我们会学得更有兴趣也更好。

《Essence of Linear Algebra》这个系列视频（16个），把重点放在线性代数的几何意义的理解上而不是计算技巧上，非常值得一看。

**矩阵就是线性变换、空间变换**

**行列式就是空间的面积或体积**

**秩就是空间的真正维度**

**行列式为0，则不是满秩，表示空间降维了**

**行列式为0，且是方阵，则是奇异矩阵；行列式不为0，且是方阵，则是非奇异矩阵**

**奇异矩阵是不可逆矩阵，没有逆矩阵；非奇异矩阵是可逆矩阵，有逆矩阵**

**如果A为奇异矩阵，则Ax=0有无穷解，Ax=b有无穷解或者无解**

**如果A为非奇异矩阵，则Ax=0有且只有唯一零解，Ax=b有唯一解**

**特征向量和特征值就是空间在矩阵变换后不变的那根轴线**



## 1、Vector向量

向量的本质是空间中的一个箭头，起点在原点，终点可以是空间中的任何一点（可以用一个数组表示）。

向量的几何意义就是n维空间中的一个有向线段，向量的计算机意义就是一个n维数组或数字列表，向量的物理意义就是从原点出发到终点的一次运动。 向量、有向线段、n维数组、一次运动，这四者之间是一一对应关系。

向量的加法，就是两个有向线段的首位叠加，两个数字列表对应项目的相加，两个运动的叠加 向量的数乘，就是对有向线段的缩放，数字列表中每个元素和外面的数字相乘，运动距离的缩放和方向的调整

向量的加法和数乘，是线性代数中最基本的两种的运算，也是所有其他概念和运算的基础，请大家记住这一点，充分理解这两个概念。

### 1.1、Base 基向量
线性代数为数据分析工作提供了一种将大量数据列表概念化、可视化的一种途径，它让数据的样式变得清晰而抽象，让人更容易理解在这些数据之上的各种运算的含义。同时，线性代数也给物理学家和计算机图形计算程序员提供了一种形式化语言，让他们能够通过计算机能处理的数字来操作空间，操作空间中的物理和图形。
基向量 Basis Vector， 也称为基base

i和j被称为一个坐标系的基向量，其中i是（1，0），j是（0，1）。
任何一个向量，比如（2，3），可以看作2*i+3*j，即对于基向量的数乘和向量和的运算组成的。
任何一个向量，在空间中的位置，取决于我们使用的基。

一个坐标系是由基向量决定的，基向量之间可以是任意夹角（非0）、任意长度（非0）。
笛卡尔坐标系是一个由垂直正相交的2或3个长度为1的基向量组成的坐标系。

## 2、Matrix矩阵

### 2.1、线性变换
线性组合（linear combination）就是两个数乘向量的和。
给定向量的线性组合所形成的向量集合，被称为给定向量张成的空间（span）。

在向量空间中，每个向量用一个箭头表示太拥挤，所以一般会用一个点来表示一个向量，并约定这个向量就是从原点开始到这个点的一个有向线段。

线性相关，是指线性组合中的某一个向量，本身是其他向量的线性组合，也就是说，这个向量落在了由其他向量张成的空间中，并没有给这个空间带来新的维度。
基的严格定义就是：空间中的一组基，是张成该空间的一组向性无关向量的集合。  线性无关（Linearly Independent）

Transformation实际上是function的一个别名，是函数的一个花哨的说法。 线性变换=线性函数=线性映射

如果线性函数是输入一个向量，并输出另一个向量，线性变换就是把向量从一个位置变到另外一个位置。

变化有很多种类，有些是非线性变换。

线性变换需要满足三个条件： 1、原点不变。 2、直线变换后仍旧是直线、而不是曲线。 3、坐标轴仍旧是等距的。
第三条实际上并不是独立的，实际上只要满足了第二条，就已经满足了第三条。

线性变换也可以定义为：保持原点不动，保持网格线平行且等距分布的变换。

### 2.2、Matrix矩阵
一个线性变换可以用两个基i和j的变化来描述，i和j放在一起组成了一个矩阵Matrix，这个矩阵，就是这个函数function。

m行n列的矩阵的每一个列，就是每一个基向量变换后的样子。所以矩阵列数表示变换前空间的维数n，矩阵列数表示变换后空间的纬度m。

**矩阵，就是线性变换的表达方式。**

也就是说：n维矩阵是对n维向量的变换。

n维矩阵*n维输入向量=n维输出向量。

**从计算的角度来看： 矩阵的本质是一个函数，一个计算工具。 向量的本质是一个函数变量，一个被处理的数据。**

当把矩阵看作是一种空间变换工具或算法的时候，就容易理解线性代数其他的概念的意义了。

矩阵根据变换效果的不同，可以分为旋转矩阵（rotation）、剪切矩阵（Shear）等等。

### 2.3、Matrix Composition 组合矩阵

一系列的变换就可以用一系列的矩阵相乘来表示（从右侧开始），成为组合矩阵。

组合矩阵是两个矩阵的相乘：M*N。表示两个线性变换相继发生后的结果，先发生N变化，后发生M变换。 所以矩阵满足组合律但是不满足交换律。

## 3、矩阵的属性

### 3.1、Determinant 行列式

如果矩阵是对一个空间的拉伸或挤压，行列式就是拉伸或挤压的程度，也就是空间中一个区域面积或体积的变化比例，或者直接用标准基围成的面积或体积的变化倍数。

行列式就是一个数字、一个标量。

如果det=0，则矩阵变换表示在降维。

在二维空间，就是空间上有两条线：
* 相交：有唯一一个解，即交点。 矩阵有逆，det！=0
* 平行：没有解。矩阵没有逆，det=0
* 重合：有无数解，即线上的每一个点。  矩阵没有逆，det=0

英文说的是det的对矩阵性质的决定性作用，中文翻译成“行列式”是在强调形状，个人认为还是英文意义更加贴切。

det（M*N）=det（M）*det（N）

### 3.2、 奇异矩阵singular matrix

如果det=0，且为方阵，则该矩阵称为奇异矩阵。

如果det不为0，且为方阵，则该矩阵称为非奇异矩阵（nonsingular matrix）。

从翻译上讲，我认为奇异矩阵更适合的叫法是“单数矩阵”，非奇异矩阵是“非单数矩阵”，非单数矩阵的表示不是单个存在的矩阵，而是存在对偶矩阵。

* 一个方阵非奇异当且仅当它的行列式不为零。
* 一个方阵非奇异当且仅当它代表的线性变换是个自同构。
* 一个矩阵半正定当且仅当它的每个特征值大于或等于零。
* 一个矩阵正定当且仅当它的每个特征值都大于零。

### 3.3、列空间ColumnSpace

列空间是矩阵所有的列所张成的空间，也就是矩阵变换前的空间在矩阵变换后的空间中的样子。

### 3.4、 秩Rank

矩阵的秩表示矩阵变换后的向量空间的维数，也就是列空间的维数。

如果矩阵的秩等于矩阵的列数，则称为“满秩”。

如果一个矩阵的秩不是满秩，那么就是说该矩阵会把空间压缩到一个更低的空间维度上，也就是说该矩阵的det=0，该矩阵是奇异矩阵。

在线性方程组中，矩阵的秩的本质就是方程组约束条件的个数。

### 3.5、 核Kernel、零空间Null Space
如果一个矩阵的秩不是满秩，那么变换后落在原点的向量的集合，被称为“零空间“或“核”。

### 3.6、InverseMatrix逆矩阵
只有方阵才有逆矩阵和非逆矩阵的概念。

矩阵*逆矩阵=单位矩阵（对角线元素为1的的对角矩阵）。

如果矩阵的det不为0，则矩阵一定存在唯一一个逆矩阵。

**线性代数除了用于操纵空间的物体或图形，另外一个应用是用于解线性方程组（ Linear System of equations）。**

解决办法是直接用逆矩阵来做线性变换。

线性方程组就是一个多元一次方程组。任何线性方程组都可以表示成“矩阵A*向量x=向量v”的形式。 Ax=v

* 如果逆矩阵的det不为0，可以保证有唯一一个解。

* 如果逆矩阵的det=0，则可能无解或者有无数多个解

### 3.7、Eigenvector特征向量、Eigenvalue特征值

用矩阵进行线性变换的时候，总会有一些向量在新的空间里角度不变或只是反向（仍旧在同一条直线上）、只是长度发生变化，那么这些向量就是这个矩阵的特征向量，长度发生的变化倍数就是特征值。

表示成公式就是：Av=av。A是矩阵，v是特征向量，a是特征值。

特征向量v不可以为零向量，所以二维空间不一定有特征向量。

一个简单的应用就是：在空间中存在一个物体，对他的矩阵变换等价于沿着特征向量形成的轴做旋转和拉伸。

另一个简单应用是：不用通过A的逆矩阵，也可以解线性方程组。

**（A-aI）v=0**

**其中：（A-aI）是奇异矩阵，I是单位矩阵**

特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。

### 3.8、奇异值SingularValue、奇异值分解SVD（singular value decomposition）
任意的矩阵 M 是可以分解成三个矩阵。V 表示了原始域的标准正交基，u 表示经过 M 变换后的co-domain的标准正交基，Σ 表示了V 中的向量与u 中相对应向量之间的关系。(V describes an orthonormal basis in the domain, and U describes an orthonormal basis in the co-domain, and Σ describes how much the vectors in V are stretched to give the vectors in U.)。 这就是奇异值分解的基础。

矩阵 M 的秩的大小等于非零奇异值的个数

特征值分解和奇异值分解的目的都是一样，就是提取出一个矩阵最重要的特征。

* 特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。
* 奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r大的奇异值来近似描述矩阵
